# ğŸŒ WEB-CRAWLER

An elite, asynchronous, high-performance web crawler built with Python â€” tailored for penetration testers, bug bounty hunters, and advanced recon workflows. It intelligently discovers hidden files, directories, keywords, and misconfigurations across large websites with speed and stealth.

<p align="center">
  <img src="https://img.shields.io/badge/Built%20With-Python%203.8%2B-blue?style=flat-square" />
  <img src="https://img.shields.io/github/license/cyber-mrinal/web-crawler?style=flat-square" />
  <img src="https://img.shields.io/badge/Status-Active-brightgreen?style=flat-square" />
</p>

---

## âš™ï¸ Features

- âœ… Asynchronous & concurrent crawling (built with asyncio & aiohttp)
- ğŸ§  Smart scope control (same domain, subdomains, or full)
- ğŸ” Keyword/Regex match on URLs, titles, and HTML
- ğŸ“ Scans sensitive paths & files (e.g. /.env, /admin, /.git)
- ğŸ“œ Optional robots.txt ignoring (for authorized scans)
- ğŸ” User-agent rotation, delays, retries & URL deduplication
- ğŸ“„ Exports results in CSV + JSON (with status, title, match info)
- ğŸ§ª Designed for offensive security use in CI or CLI pipelines
- â˜ ï¸ Graceful Ctrl+C stop (saves partial data)

---

## ğŸš€ Quick Start

1. Clone the repository:

```bash
git clone https://github.com/CYBER-MRINAL/WEB-CRAWLER.git
cd WEB-CRAWLER
````

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Run the crawler:

```bash
python3 elite_crawler.py https://example.com \
  --max-depth 3 \
  --output scan_results \
  --keywords admin login password \
  --concurrency 20 \
  --delay 0.3 \
  --url-budget 1000
```

---

## ğŸ“Œ Command-Line Options

| Argument        | Description                                             |
| --------------- | ------------------------------------------------------- |
| url             | Target base URL to crawl                                |
| --max-depth     | How deep to follow links (default: 3)                   |
| --output        | Output file prefix (e.g., results â†’ results.csv, .json) |
| --keywords      | Keywords or regex to trigger alerts (e.g., admin login) |
| --user-agents   | Custom user-agent list                                  |
| --concurrency   | Number of concurrent requests (default: 15)             |
| --delay         | Delay between requests (seconds)                        |
| --url-budget    | Stop after N total URLs crawled                         |
| --domain-scope  | Scope control: same, subdomains, or all                 |
| --ignore-robots | Ignore robots.txt (for authorized targets only)         |

---

<img width="1748" height="989" alt="image" src="https://github.com/user-attachments/assets/c1b0b5ba-b93e-4e67-8586-a0e1b28f9752" />

--- 

## ğŸ¯ Example Use Cases

* âœ… Recon and discovery in bug bounty programs
* ğŸ” Internal web asset scanning for exposed secrets
* ğŸ›¡ï¸ Red teaming infrastructure enumeration
* ğŸ§ª CI-integrated automated recon scans

---

## ğŸ›  Tech Stack

* ğŸ Python 3.8+
* âš™ï¸ aiohttp / asyncio â€“ fast, async HTTP client
* ğŸ§  BeautifulSoup â€“ HTML parsing
* ğŸ¨ Colorama â€“ CLI colors
* ğŸ§° argparse â€“ CLI argument parsing

---

## ğŸ’¡ Tips

* Set User-Agent strings that match your engagement type.
* Rotate proxies for stealth crawling.
* Use --url-budget to avoid infinite loops.
* Use --keywords with regex like 'api\_key|password|token' to find secrets.
* Always scan ethically and with permission.

---

## ğŸ“¦ Output Format

* results.csv â†’ Contains: url, status, title, hit
* results.json â†’ Full structured output for integration

Example:

```json
[
  {
    "url": "https://example.com/admin",
    "status": 200,
    "title": "Admin Panel",
    "hit": true
  }
]
```

---

## ğŸ§‘â€ğŸ’» Developer Setup

Install manually without setup script:

```bash
pip install aiohttp beautifulsoup4 colorama
```

Make executable:

```bash
chmod +x elite_crawler.py
./elite_crawler.py https://example.com --max-depth 2 --output scan
```

---

## ğŸ§¯ Troubleshooting

| Problem           | Fix                                                                 |
| ----------------- | ------------------------------------------------------------------- |
| ImportError       | Re-run pip install requirements.txt                                 |
| Permission denied | chmod +x elite\_crawler.py                                          |
| Blocked URLs      | Check robots.txt or use --ignore-robots (for authorized scans only) |
| SSL errors        | Add --insecure flag (planned) or verify target cert                 |
| Nothing crawled   | Check base URL syntax and depth / scope limits                      |

---

## ğŸ¤ Community & Support

For discussions, help, or suggestions:

ğŸ”— Telegram Group â†’ [Cyber Mrinal Group](https://t.me/cybermrinalgroup/3)

ğŸ“¬ GitHub Issues â†’ Submit feature requests or bugs

---

## ğŸ›¡ï¸ Legal & Ethics Notice

This tool is provided for legal penetration testing, bug bounty research, and authorized reconnaissance.

âš ï¸ Do not scan any target without explicit permission.

Author is not responsible for misuse of this tool.

---

## ğŸ“„ License

MIT License â€” feel free to modify, reuse, and contribute.

---

Made with ğŸ by [CYBER-MRINAL](https://github.com/CYBER-MRINAL)

